
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="The theoretical building blocks. "How is the model built?"">
      
      
        <meta name="author" content="Saurabh Goyal">
      
      
      
        <link rel="prev" href="../../model_architecture_families/mixture_of_experts/">
      
      
        <link rel="next" href="../mqa/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.23">
    
    
      
        <title>Multi-Head Attention - LLM-Architecture_Foundations</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Inter:300,300i,400,400i,700,700i%7CJetBrains+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Inter";--md-code-font:"JetBrains Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-overview" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="LLM-Architecture_Foundations" class="md-header__button md-logo" aria-label="LLM-Architecture_Foundations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            LLM-Architecture_Foundations
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Multi-Head Attention
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="blue" data-md-color-accent="deep-purple"  aria-hidden="true"  type="radio" name="__palette" id="__palette_0">
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="blue" data-md-color-accent="lime"  aria-hidden="true"  type="radio" name="__palette" id="__palette_1">
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/saurabhiit2007/LLM-Architecture-Foundations" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="LLM-Architecture_Foundations" class="md-nav__button md-logo" aria-label="LLM-Architecture_Foundations" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    LLM-Architecture_Foundations
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/saurabhiit2007/LLM-Architecture-Foundations" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Model Architecture Families
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Model Architecture Families
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture_families/encoder_decoder_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Encoder Decoder Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture_families/dense_vs_sparse_models/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dense vs Sparse Models
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../model_architecture_families/mixture_of_experts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mixture of Models (MoE)
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Attention Mechanisms
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Attention Mechanisms
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Multi-Head Attention
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Multi-Head Attention
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      1. Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-mathematical-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      2. The Mathematical Mechanism
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. The Mathematical Mechanism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-step-by-step-flow" class="md-nav__link">
    <span class="md-ellipsis">
      The Step-by-Step Flow
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-advanced-concepts-recent-updates" class="md-nav__link">
    <span class="md-ellipsis">
      3. Advanced Concepts &amp; Recent Updates
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Advanced Concepts &amp; Recent Updates">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-grouped-query-attention-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      A. Grouped-Query Attention (GQA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-flashattention-v2v3" class="md-nav__link">
    <span class="md-ellipsis">
      B. FlashAttention (v2/v3)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-rotary-positional-embeddings-rope" class="md-nav__link">
    <span class="md-ellipsis">
      C. Rotary Positional Embeddings (RoPE)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-interview-questions" class="md-nav__link">
    <span class="md-ellipsis">
      4. Interview Questions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Interview Questions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q1-why-is-the-scaling-factor-sqrtd_k-necessary" class="md-nav__link">
    <span class="md-ellipsis">
      Q1: Why is the scaling factor \(\sqrt{d_k}\) necessary?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-dot-product-attention-recap" class="md-nav__link">
    <span class="md-ellipsis">
      1. Dot product attention recap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-why-dot-products-grow-with-dimension" class="md-nav__link">
    <span class="md-ellipsis">
      2. Why dot products grow with dimension
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-effect-on-softmax-without-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      3. Effect on softmax without scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-how-scaling-by-d_k-helps" class="md-nav__link">
    <span class="md-ellipsis">
      4. How scaling by √d_k helps
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-numerical-example" class="md-nav__link">
    <span class="md-ellipsis">
      5. Numerical example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Numerical example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#case-1-small-dimension-d_k-4" class="md-nav__link">
    <span class="md-ellipsis">
      Case 1: Small dimension (d_k = 4)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#case-2-larger-dimension-d_k-64" class="md-nav__link">
    <span class="md-ellipsis">
      Case 2: Larger dimension (d_k = 64)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q2-what-is-the-difference-between-multi-head-mha-multi-query-mqa-and-grouped-query-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      Q2: What is the difference between Multi-Head (MHA), Multi-Query (MQA), and Grouped-Query (GQA)?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q3-how-does-the-complexity-change-if-we-double-the-sequence-length-n" class="md-nav__link">
    <span class="md-ellipsis">
      Q3: How does the complexity change if we double the sequence length (\(N\))?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q4-can-multi-head-attention-be-computed-in-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Q4: Can Multi-Head Attention be computed in parallel?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mqa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Multi Query Attention (MQA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gqa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Grouped Query Attention (GQA)
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../sliding_window/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sliding Window
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Real World Model Families
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Real World Model Families
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../real_world_model_families/claude/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Claude
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../real_world_model_families/llama/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Llama
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../real_world_model_families/gemini/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Gemini
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../real_world_model_families/openai/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    OpenAI
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../real_world_model_families/mistral/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mistral
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../real_world_model_families/dense_vs_MoE_comparison/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Dense vs MoE Comparison
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../real_world_model_families/general_comparison/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    General Comparison
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Transformer Components
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            Transformer Components
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Tokenization
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            Tokenization
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/unigram/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Unigram
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/sentence_piece_unigram/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Sentence Piece Unigram
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/byte_pair_encoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Byte Pair Encoding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/tiktoken/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    TikToken
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../tokenization/vocabulary_size_tradeoffs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Vocabulary Size Tradeoffs
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_7" >
        
          
          <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Positional Encodings
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            Positional Encodings
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../positional_encodings/RoPE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RoPE
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../positional_encodings/ALiBi/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ALiBi
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_8" >
        
          
          <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Pretraining objective
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_8">
            <span class="md-nav__icon md-icon"></span>
            Pretraining objective
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_9" >
        
          
          <label class="md-nav__link" for="__nav_9" id="__nav_9_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    Scaling laws
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_9">
            <span class="md-nav__icon md-icon"></span>
            Scaling laws
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../references.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    References
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-overview" class="md-nav__link">
    <span class="md-ellipsis">
      1. Overview
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-the-mathematical-mechanism" class="md-nav__link">
    <span class="md-ellipsis">
      2. The Mathematical Mechanism
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. The Mathematical Mechanism">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-step-by-step-flow" class="md-nav__link">
    <span class="md-ellipsis">
      The Step-by-Step Flow
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-advanced-concepts-recent-updates" class="md-nav__link">
    <span class="md-ellipsis">
      3. Advanced Concepts &amp; Recent Updates
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Advanced Concepts &amp; Recent Updates">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-grouped-query-attention-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      A. Grouped-Query Attention (GQA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-flashattention-v2v3" class="md-nav__link">
    <span class="md-ellipsis">
      B. FlashAttention (v2/v3)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#c-rotary-positional-embeddings-rope" class="md-nav__link">
    <span class="md-ellipsis">
      C. Rotary Positional Embeddings (RoPE)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-interview-questions" class="md-nav__link">
    <span class="md-ellipsis">
      4. Interview Questions
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Interview Questions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#q1-why-is-the-scaling-factor-sqrtd_k-necessary" class="md-nav__link">
    <span class="md-ellipsis">
      Q1: Why is the scaling factor \(\sqrt{d_k}\) necessary?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#1-dot-product-attention-recap" class="md-nav__link">
    <span class="md-ellipsis">
      1. Dot product attention recap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-why-dot-products-grow-with-dimension" class="md-nav__link">
    <span class="md-ellipsis">
      2. Why dot products grow with dimension
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-effect-on-softmax-without-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      3. Effect on softmax without scaling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-how-scaling-by-d_k-helps" class="md-nav__link">
    <span class="md-ellipsis">
      4. How scaling by √d_k helps
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-numerical-example" class="md-nav__link">
    <span class="md-ellipsis">
      5. Numerical example
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Numerical example">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#case-1-small-dimension-d_k-4" class="md-nav__link">
    <span class="md-ellipsis">
      Case 1: Small dimension (d_k = 4)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#case-2-larger-dimension-d_k-64" class="md-nav__link">
    <span class="md-ellipsis">
      Case 2: Larger dimension (d_k = 64)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q2-what-is-the-difference-between-multi-head-mha-multi-query-mqa-and-grouped-query-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      Q2: What is the difference between Multi-Head (MHA), Multi-Query (MQA), and Grouped-Query (GQA)?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q3-how-does-the-complexity-change-if-we-double-the-sequence-length-n" class="md-nav__link">
    <span class="md-ellipsis">
      Q3: How does the complexity change if we double the sequence length (\(N\))?
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#q4-can-multi-head-attention-be-computed-in-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Q4: Can Multi-Head Attention be computed in parallel?
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>Multi-Head Attention</h1>

<h2 id="1-overview">1. Overview<a class="headerlink" href="#1-overview" title="Permanent link">&para;</a></h2>
<p>Multi-Head Attention is the fundamental mechanism that allows Transformers to focus on different parts of an input sequence simultaneously. Instead of one "viewpoint," the model gets multiple parallel perspectives.</p>
<hr />
<hr />
<h2 id="2-the-mathematical-mechanism">2. The Mathematical Mechanism<a class="headerlink" href="#2-the-mathematical-mechanism" title="Permanent link">&para;</a></h2>
<p>The MHA process involves transforming input embeddings into three distinct spaces: <strong>Queries (Q)</strong>, <strong>Keys (K)</strong>, and <strong>Values (V)</strong>.</p>
<h3 id="the-step-by-step-flow">The Step-by-Step Flow<a class="headerlink" href="#the-step-by-step-flow" title="Permanent link">&para;</a></h3>
<ol>
<li><strong>Linear Projection</strong>: For <span class="arithmatex">\(h\)</span> heads, the input <span class="arithmatex">\(X\)</span> is projected using learned weights <span class="arithmatex">\(W_i^Q, W_i^K, W_i^V\)</span>.</li>
<li><strong>Scaled Dot-Product Attention</strong>: Each head computes attention independently:
   <span class="arithmatex">\(<span class="arithmatex">\(\text{Head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\)</span>\)</span></li>
<li><strong>Concatenation</strong>: All heads are joined: <span class="arithmatex">\(\text{Concat}(\text{Head}_1, ..., \text{Head}_h)\)</span>.</li>
<li><strong>Final Output Projection</strong>: The result is multiplied by <span class="arithmatex">\(W^O\)</span> to return to the original model dimension.</li>
</ol>
<hr />
<hr />
<h2 id="3-advanced-concepts-recent-updates">3. Advanced Concepts &amp; Recent Updates<a class="headerlink" href="#3-advanced-concepts-recent-updates" title="Permanent link">&para;</a></h2>
<p>If you are interviewing for a Senior or Research role, these updates are critical:</p>
<h3 id="a-grouped-query-attention-gqa">A. Grouped-Query Attention (GQA)<a class="headerlink" href="#a-grouped-query-attention-gqa" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>The Context</strong>: Standard MHA is slow during inference because the "KV Cache" grows too large for GPU memory.</li>
<li><strong>The Innovation</strong>: Instead of every Query head having its own Key/Value head, <strong>GQA</strong> shares one KV head among a group of Query heads.</li>
<li><strong>Why it matters</strong>: It’s the standard for <strong>Llama 3</strong> and <strong>Mistral</strong>, offering a perfect balance between speed and accuracy.</li>
</ul>
<h3 id="b-flashattention-v2v3">B. FlashAttention (v2/v3)<a class="headerlink" href="#b-flashattention-v2v3" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>The Context</strong>: The <span class="arithmatex">\(O(N^2)\)</span> complexity of attention makes long sequences (like books) hard to process.</li>
<li><strong>The Innovation</strong>: It utilizes "Tiling" to compute attention in blocks within the GPU's fast SRAM, avoiding the bottleneck of slower main memory (HBM).</li>
<li><strong>Interview Tip</strong>: If asked how to scale Transformers to 100k+ tokens, mention <strong>FlashAttention</strong> and <strong>GQA</strong>.</li>
</ul>
<h3 id="c-rotary-positional-embeddings-rope">C. Rotary Positional Embeddings (RoPE)<a class="headerlink" href="#c-rotary-positional-embeddings-rope" title="Permanent link">&para;</a></h3>
<ul>
<li>Modern MHA (used in Llama) uses <strong>RoPE</strong> instead of additive positional encoding. It encodes position by rotating the Q and K vectors in a complex space, which naturally captures the <em>relative</em> distance between tokens.</li>
</ul>
<hr />
<hr />
<h2 id="4-interview-questions">4. Interview Questions<a class="headerlink" href="#4-interview-questions" title="Permanent link">&para;</a></h2>
<h3 id="q1-why-is-the-scaling-factor-sqrtd_k-necessary">Q1: Why is the scaling factor <span class="arithmatex">\(\sqrt{d_k}\)</span> necessary?<a class="headerlink" href="#q1-why-is-the-scaling-factor-sqrtd_k-necessary" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> The scaling factor √d_k in scaled dot product attention is used to keep attention scores numerically stable and to prevent the softmax from becoming overly sharp as the dimensionality of the keys and queries increases.</p>
<p><strong>Detailed Explanation</strong></p>
<blockquote>
<h3 id="1-dot-product-attention-recap">1. Dot product attention recap<a class="headerlink" href="#1-dot-product-attention-recap" title="Permanent link">&para;</a></h3>
<p>n dot product attention, the raw attention score between a query vector q and a key vector k is computed as:</p>
<p>$$
    q \cdot k = \sum_{i=1}^{d_k} q_i k_i
    $$</p>
<p>hese scores are then passed through a softmax to obtain attention weights.</p>
<p>Softmax is highly sensitive to the magnitude of its inputs. Larger values lead to very peaked distributions, while smaller values lead to smoother distributions.</p>
<hr />
<h3 id="2-why-dot-products-grow-with-dimension">2. Why dot products grow with dimension<a class="headerlink" href="#2-why-dot-products-grow-with-dimension" title="Permanent link">&para;</a></h3>
<p>Assume a standard training scenario:</p>
<ul>
<li>Each component of q and k has zero mean</li>
<li>Each component has variance 1</li>
<li>Components are independent</li>
</ul>
<p>Each term <span class="arithmatex">\(q_i k_i\)</span> has:
- Mean 0
- Variance 1</p>
<p>Since the dot product is the sum of <span class="arithmatex">\(d_k\)</span> such terms, its variance is:</p>
<div class="arithmatex">\[
\text{Var}(q \cdot k) = d_k
\]</div>
<p>This means the typical magnitude of the dot product grows proportionally to:</p>
<div class="arithmatex">\[
\sqrt{d_k}
\]</div>
<p>As a result, simply increasing the dimensionality increases attention scores even if the semantic similarity stays the same.</p>
<hr />
<h3 id="3-effect-on-softmax-without-scaling">3. Effect on softmax without scaling<a class="headerlink" href="#3-effect-on-softmax-without-scaling" title="Permanent link">&para;</a></h3>
<p>As attention logits grow larger:</p>
<ul>
<li>Softmax outputs become close to one hot vectors</li>
<li>One key dominates the attention</li>
<li>Gradients through the softmax become very small</li>
</ul>
<p>This leads to slow learning and unstable optimization, especially in early training.</p>
<hr />
<h3 id="4-how-scaling-by-d_k-helps">4. How scaling by √d_k helps<a class="headerlink" href="#4-how-scaling-by-d_k-helps" title="Permanent link">&para;</a></h3>
<p>Scaled dot product attention divides the dot product by √d_k:</p>
<div class="arithmatex">\[
\frac{q \cdot k}{\sqrt{d_k}}
\]</div>
<p>This normalizes the variance:</p>
<div class="arithmatex">\[
\text{Var}\left(\frac{q \cdot k}{\sqrt{d_k}}\right) = 1
\]</div>
<p>As a result, attention logits have a consistent scale regardless of the dimensionality, keeping softmax in an effective operating range.</p>
<p>This idea is closely related to variance preserving techniques such as Xavier initialization.</p>
<hr />
<h3 id="5-numerical-example">5. Numerical example<a class="headerlink" href="#5-numerical-example" title="Permanent link">&para;</a></h3>
<h4 id="case-1-small-dimension-d_k-4">Case 1: Small dimension (d_k = 4)<a class="headerlink" href="#case-1-small-dimension-d_k-4" title="Permanent link">&para;</a></h4>
<p>Typical dot product magnitude:
$$
\sqrt{4} = 2
$$</p>
<p>Logits like:
$$
[2.1, 1.8, 2.4]
$$</p>
<p>Softmax remains relatively smooth.</p>
<hr />
<h4 id="case-2-larger-dimension-d_k-64">Case 2: Larger dimension (d_k = 64)<a class="headerlink" href="#case-2-larger-dimension-d_k-64" title="Permanent link">&para;</a></h4>
<p>Typical dot product magnitude:
$$
\sqrt{64} = 8
$$</p>
<p>Logits might look like:
$$
[8.3, 7.9, 8.7]
$$</p>
<p>Softmax becomes very sharp, even though relative differences are similar.</p>
<p>After scaling:
$$
\frac{[8.3, 7.9, 8.7]}{8} \approx [1.04, 0.99, 1.09]
$$</p>
<p>Softmax behavior is now comparable to the small dimension case.</p>
<hr />
</blockquote>
<h3 id="q2-what-is-the-difference-between-multi-head-mha-multi-query-mqa-and-grouped-query-gqa">Q2: What is the difference between Multi-Head (MHA), Multi-Query (MQA), and Grouped-Query (GQA)?<a class="headerlink" href="#q2-what-is-the-difference-between-multi-head-mha-multi-query-mqa-and-grouped-query-gqa" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>MHA</strong>: Each Query head has a unique Key and Value head. (High memory, high quality).</li>
<li><strong>MQA</strong>: All Query heads share a single Key and Value head. (Low memory, lower quality).</li>
<li><strong>GQA</strong>: Query heads are partitioned into groups; each group shares one KV head. (Optimal balance).</li>
</ul>
<h3 id="q3-how-does-the-complexity-change-if-we-double-the-sequence-length-n">Q3: How does the complexity change if we double the sequence length (<span class="arithmatex">\(N\)</span>)?<a class="headerlink" href="#q3-how-does-the-complexity-change-if-we-double-the-sequence-length-n" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Because attention is <span class="arithmatex">\(O(N^2)\)</span>, doubling the sequence length quadruples the computational cost and memory requirement. This is the "Quadratic Bottleneck."</p>
<h3 id="q4-can-multi-head-attention-be-computed-in-parallel">Q4: Can Multi-Head Attention be computed in parallel?<a class="headerlink" href="#q4-can-multi-head-attention-be-computed-in-parallel" title="Permanent link">&para;</a></h3>
<p><strong>Answer:</strong> Yes. Unlike RNNs which are sequential, all heads in MHA can be computed simultaneously. Furthermore, the attention for all tokens in a sequence can be computed in parallel during training.</p>
<hr />












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.expand", "navigation.top", "search.highlight", "search.share", "content.code.copy", "mathjax"], "search": "../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>