{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"attention_mechanisms/gqa/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"attention_mechanisms/gqa/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"attention_mechanisms/gqa/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"attention_mechanisms/mqa/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"attention_mechanisms/mqa/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"attention_mechanisms/mqa/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"attention_mechanisms/sliding_window/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"attention_mechanisms/sliding_window/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"attention_mechanisms/sliding_window/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"model_architecture_families/dense_vs_sparse_models/","title":"Dense vs Sparse Models","text":""},{"location":"model_architecture_families/dense_vs_sparse_models/#1-high-level-concept","title":"1. High-Level Concept","text":"<p>The fundamental difference between Dense and Sparse models lies in the relationship between Active Parameters and Total Parameters.</p> <ul> <li> <p>Dense Models (e.g., LLaMA 3, GPT-3):   All model parameters are activated for every token. Increasing model capacity directly increases per-token compute and memory cost.</p> </li> <li> <p>Sparse Models / Mixture of Experts (MoE) (e.g., Mixtral, some DeepSeek variants):   The model contains a large pool of parameters, but only a small subset is activated per token. This enables scaling model capacity faster than per-token compute, although system-level costs still increase.</p> </li> </ul> <p>Note: MoE decouples total parameter count from per-token compute, but does not eliminate compute, memory, or system overhead.</p>"},{"location":"model_architecture_families/dense_vs_sparse_models/#2-architecture-comparison","title":"2. Architecture Comparison","text":""},{"location":"model_architecture_families/dense_vs_sparse_models/#21-dense-transformer-block","title":"2.1 Dense Transformer Block","text":"<p>In a standard Transformer layer, each token passes through:</p> <ol> <li>Self-Attention</li> <li>A single dense Feed-Forward Network (FFN)</li> </ol> <p>All FFN parameters are evaluated for every token.</p>"},{"location":"model_architecture_families/dense_vs_sparse_models/#22-sparse-moe-transformer-block","title":"2.2 Sparse (MoE) Transformer Block","text":"<p>MoE replaces the single FFN with:</p> <ol> <li> <p>Experts:    A set of \\(N\\) independent FFNs, commonly ranging from 8 to 64 experts.</p> </li> <li> <p>Router (Gating Network):    A lightweight network that selects which experts should process each token.</p> </li> </ol> <p>Important architectural notes:</p> <ul> <li>MoE typically replaces only the FFN, not the attention mechanism.</li> <li>Many architectures interleave dense and MoE layers rather than applying MoE everywhere.</li> </ul> <p>Mathematical Formulation</p> <p>For an input token \\(x\\), the output is a weighted combination of the selected experts:</p> \\[ y = \\sum_{i \\in \\text{TopK}} G(x)_i \\cdot E_i(x) \\] <p>Where: - \\(G(x)_i\\) is the routing probability for expert \\(i\\) - \\(E_i(x)\\) is the output of expert \\(i\\) - Most models use Top-2 routing during training and often Top-1 routing during inference</p> <p>Note: Detailed description of MoE is available here</p>"},{"location":"model_architecture_families/dense_vs_sparse_models/#3-dense-vs-sparse-comparison-table","title":"3. Dense vs. Sparse Comparison Table","text":"Feature Dense Models Sparse Models (MoE) Per-Token Compute Proportional to model size Proportional to active experts Total Parameters Equal to active parameters Much larger than active parameters Memory Footprint Proportional to total parameters Proportional to total parameters plus routing overhead Training Complexity Stable and well understood Complex due to routing and load balancing Inference Latency Compute-bound Often communication-bound Scaling Behavior Performance scales with compute Performance scales with total capacity <p>Note: - MoE reduces FLOPs per token but can increase system overhead. - Dense models are typically compute-bound, while MoE models are often bandwidth-bound.</p>"},{"location":"model_architecture_families/dense_vs_sparse_models/#4-key-engineering-challenges","title":"4. Key Engineering Challenges","text":""},{"location":"model_architecture_families/dense_vs_sparse_models/#41-load-balancing-and-auxiliary-losses","title":"4.1 Load Balancing and Auxiliary Losses","text":"<p>Problem:</p> <p>Routers tend to over-select a small subset of experts, leading to expert collapse and under-trained experts.</p> <p>Solution:</p> <p>Auxiliary losses encourage uniform expert utilization by penalizing skewed token distributions. These are often referred to as load balancing or importance losses.</p> <p>Common failure modes:</p> <ul> <li>Expert collapse</li> <li>Token dropping due to expert capacity limits</li> </ul>"},{"location":"model_architecture_families/dense_vs_sparse_models/#42-expert-specialization","title":"4.2 Expert Specialization","text":"<ul> <li>Experts rarely align with human-interpretable domains such as coding or specific languages.</li> <li>In practice, experts specialize in syntactic patterns, token statistics, or latent semantic clusters.</li> <li>Some experts may become partially redundant.</li> </ul> <p>Specialization is emergent rather than explicitly supervised.</p>"},{"location":"model_architecture_families/dense_vs_sparse_models/#43-system-design-and-communication-overhead","title":"4.3 System Design and Communication Overhead","text":"<p>In distributed training or inference:</p> <ul> <li>Tokens are sharded across devices.</li> <li>Experts are distributed across GPUs or nodes.</li> <li>Tokens frequently require all-to-all communication to reach the selected experts.</li> </ul> <p>Therefore, Sparse models are often limited by network bandwidth and communication latency rather than raw FLOPs.</p> <p>During inference, systems may reduce communication overhead using:</p> <ul> <li>Static expert placement</li> <li>Expert parallelism</li> <li>Reduced routing flexibility</li> </ul>"},{"location":"model_architecture_families/dense_vs_sparse_models/#5-summary-of-pros-and-cons","title":"5. Summary of Pros and Cons","text":""},{"location":"model_architecture_families/dense_vs_sparse_models/#51-pros-of-sparse-moe","title":"5.1 Pros of Sparse (MoE)","text":"<ul> <li>Higher model capacity at similar per-token compute</li> <li>Improved scaling efficiency at very large parameter counts</li> <li>Lower training compute for a given performance target</li> </ul>"},{"location":"model_architecture_families/dense_vs_sparse_models/#52-cons-of-sparse-moe","title":"5.2 Cons of Sparse (MoE)","text":"<ul> <li>Memory usage scales with total parameters, not active parameters</li> <li>Complex training dynamics and sensitivity to routing hyperparameters</li> <li>Higher system and serving complexity</li> <li>Inference latency can be dominated by communication overhead</li> </ul> <p>Example: A 47B MoE model with 12B active parameters still requires memory comparable to a 47B dense model.</p>"},{"location":"model_architecture_families/encoder_decoder_models/","title":"Encoder Decoder Models","text":""},{"location":"model_architecture_families/encoder_decoder_models/#1-overview","title":"1. Overview","text":"<p>Mixture of Experts (MoE) is an architectural paradigm that enables scaling model capacity to frontier levels while keeping per-token inference compute manageable. It allows a model to store far more knowledge than a dense model with similar inference cost, making it a key technique behind models such as GPT-4, Mixtral, and Grok.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#2-core-concept-and-intuition","title":"2. Core Concept and Intuition","text":"<p>In a standard dense Transformer, every parameter participates in processing every token.</p> <p>The problem with dense scaling</p> <ul> <li>Increasing parameters increases capacity</li> <li>But inference cost, latency, and memory usage scale linearly with model size</li> </ul> <p>The MoE solution</p> <p>MoE decouples model capacity from inference compute by activating only a small subset of parameters for each token.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#the-specialist-analogy","title":"The Specialist Analogy","text":"<p>Instead of one generalist handling all tasks, imagine a panel of specialists.</p> <ul> <li>A routing system decides which specialists should handle each input</li> <li>Only those specialists are consulted</li> </ul> <p>Key distinction:</p> <ul> <li>Total parameters represent the full knowledge capacity</li> <li>Active parameters determine inference cost for a given token</li> </ul>"},{"location":"model_architecture_families/encoder_decoder_models/#3-architecture-the-sparse-transformer","title":"3. Architecture: The Sparse Transformer","text":"<p>An MoE model is identical to a standard Transformer except that the Feed-Forward Network (FFN) layers are replaced with MoE layers.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#components-of-an-moe-layer","title":"Components of an MoE Layer","text":"<ol> <li>Experts (\\(E_i\\)) </li> </ol> <p>A set of \\(N\\) independent FFNs, each with its own parameters.</p> <ol> <li>Router / Gating Network (\\(G\\)) </li> </ol> <p>A small learnable function that scores which experts should process a given token.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#routing-mechanism","title":"Routing Mechanism","text":"<p>For an input token representation \\(x\\), the output of an MoE layer is:</p> \\[ y = \\sum_{i=1}^{N} G(x)_i \\cdot E_i(x) \\] <p>In sparse MoE, a Top-k routing strategy is used:</p> <ul> <li>Only the top \\(k\\) experts receive non-zero weights</li> <li>All other experts are skipped entirely</li> <li>Typically \\(k = 1\\) or \\(k = 2\\)</li> </ul> <p>Only the selected experts are evaluated, making computation and gradient flow sparse.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#case-study-mixtral-8x7b","title":"Case Study: Mixtral 8x7B","text":"<ul> <li>Total experts: 8</li> <li>Routing: Top-2 per token</li> <li>Active parameters per token: ~13B</li> <li>Total parameters: ~47B</li> </ul> <p>The model exhibits capacity comparable to a ~50B dense model while running at the speed of a ~13B model.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#4-expert-capacity-and-token-dropping","title":"4. Expert Capacity and Token Dropping","text":"<p>Each expert has a fixed capacity, which limits how many tokens it can process in a single batch. This capacity is typically set as a multiple of the expected average load per expert.</p> <p>If too many tokens are routed to the same expert in a batch:</p> <ul> <li>Excess tokens may be dropped entirely</li> <li>Or processed with reduced routing weight, meaning their contribution to the expert\u2019s output is scaled down to maintain numerical and compute stability</li> <li>Or rerouted to fallback experts, depending on the implementation</li> </ul> <p>This mechanism prevents individual experts from becoming compute or memory bottlenecks but introduces a trade-off:</p> <ul> <li>Larger capacity improves training stability and model quality</li> <li>Smaller capacity improves efficiency but risks silent quality degradation due to dropped tokens</li> </ul> <p>Monitoring expert utilization and token dropping rates is therefore critical during training and debugging MoE models.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#why-reduce-routing-weight-even-when-the-expert-is-correct","title":"Why Reduce Routing Weight Even When the Expert Is Correct?","text":"<p>Routing decides which expert is best for a token. Capacity limits decide how much influence that token is allowed to have in a given batch.</p> <p>When an expert exceeds its capacity, all routed tokens are still correct assignments, but the system cannot afford to:</p> <ul> <li>Process unlimited tokens</li> <li>Accumulate unbounded gradients</li> <li>Let one expert dominate training</li> </ul> <p>Reducing the routing weight is a soft fallback:</p> <ul> <li>The token is still processed by the correct expert</li> <li>Its output and gradients are scaled down</li> <li>Compute and training stability are preserved</li> </ul> <p>The reduced weight does not indicate lower correctness. It limits influence to protect compute budgets and prevent expert collapse while retaining partial learning signal.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#5-training-dynamics-and-stability","title":"5. Training Dynamics and Stability","text":""},{"location":"model_architecture_families/encoder_decoder_models/#benefits-of-moe-training","title":"Benefits of MoE Training","text":"<ul> <li>Compute efficiency: Lower validation loss for the same training FLOPs compared to dense models</li> <li>Knowledge scaling: Experts can store long-tail facts and rare patterns efficiently</li> <li>Faster convergence: Sparse FFNs reduce redundant computation</li> </ul>"},{"location":"model_architecture_families/encoder_decoder_models/#mode-collapse-and-expert-imbalance","title":"Mode Collapse and Expert Imbalance","text":"<p>A common failure mode is expert collapse:</p> <ul> <li>Early-random advantages cause one expert to receive more tokens</li> <li>That expert improves faster due to more gradients</li> <li>Other experts receive fewer updates and remain undertrained</li> </ul>"},{"location":"model_architecture_families/encoder_decoder_models/#auxiliary-losses-for-stability","title":"Auxiliary Losses for Stability","text":"<p>To prevent collapse, MoE training includes additional losses:</p> <ul> <li>Load Balancing Loss: Penalizes uneven token distribution across experts</li> <li>Z-Loss: Penalizes large router logits to improve numerical stability</li> </ul> <p>These losses are essential for maintaining expert diversity.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#5-emergent-expert-specialization","title":"5. Emergent Expert Specialization","text":"<p>Experts are not manually assigned domains.</p> <p>Specialization emerges implicitly from:</p> <ul> <li>Routing gradients</li> <li>Data distribution</li> <li>Load balancing constraints</li> </ul> <p>In practice, experts often specialize in:</p> <ul> <li>Syntax and formatting</li> <li>Punctuation and boilerplate</li> <li>Code versus natural language</li> <li>Long-context versus short-context tokens</li> </ul> <p>MoE does not guarantee clean semantic specialization such as math or biology experts.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#6-what-moe-improves-and-what-it-does-not","title":"6. What MoE Improves and What It Does Not","text":""},{"location":"model_architecture_families/encoder_decoder_models/#moe-primarily-improves","title":"MoE Primarily Improves","text":"<ul> <li>Factual recall</li> <li>Coverage of rare or long-tail patterns</li> <li>Knowledge density per inference FLOP</li> </ul>"},{"location":"model_architecture_families/encoder_decoder_models/#moe-does-not-automatically-improve","title":"MoE Does Not Automatically Improve","text":"<ul> <li>Multi-step reasoning</li> <li>Logical consistency</li> <li>Planning and abstraction</li> </ul> <p>Reasoning quality depends more on:</p> <ul> <li>Attention mechanisms</li> <li>Data quality</li> <li>Post-training alignment and RL</li> </ul>"},{"location":"model_architecture_families/encoder_decoder_models/#7-inference-and-deployment-trade-offs","title":"7. Inference and Deployment Trade-offs","text":"Aspect Impact Throughput High, due to sparse computation Latency Low, driven by active parameter count VRAM Usage Very high, since all experts must be resident Communication High, requires all-to-all routing in distributed setups <p>MoE models are often memory-bandwidth bound, not compute-bound.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#8-training-cost-vs-inference-cost","title":"8. Training Cost vs Inference Cost","text":"<p>MoE reduces inference cost but increases training complexity:</p> <ul> <li>More communication overhead</li> <li>More fragile optimization</li> <li>Harder distributed orchestration</li> </ul> <p>MoE is most effective when:</p> <ul> <li>A model is trained once</li> <li>Served at massive scale</li> <li>Inference cost dominates total lifetime cost</li> </ul> <p>Dense models may be preferable for smaller-scale or latency-critical use cases.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#9-moe-in-the-scaling-toolbox","title":"9. MoE in the Scaling Toolbox","text":"Strategy Key Idea Trade-off Dense scaling Increase parameters Expensive inference MoE Sparse activation Memory and communication overhead Longer training More tokens per parameter Higher one-time cost Quantization Lower precision Potential accuracy loss <p>MoE is a powerful but specialized tool, not a universal solution.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#10-key-takeaways","title":"10. Key Takeaways","text":"<ul> <li>MoE decouples capacity from inference compute</li> <li>It is most effective for knowledge-heavy scaling</li> <li>Training is harder, inference is cheaper</li> <li>Many failures stem from routing imbalance and systems constraints</li> </ul> <p>MoE reflects a broader trend in modern LLMs: scaling is as much a systems problem as it is a modeling problem.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#11-some-questions","title":"11. Some Questions","text":"<p>Q: Does MoE reduce attention bottlenecks? A: No. MoE typically replaces FFN layers. Attention remains dense, so KV cache memory and attention compute are unchanged.</p> <p>Q: Why use Top-2 routing instead of Top-1? A: Top-2 provides smoother gradients and backup information flow, improving training stability.</p> <p>Q: What is the main bottleneck when serving MoE models? A: Memory bandwidth and communication, not raw FLOPs.</p>"},{"location":"model_architecture_families/encoder_decoder_models/#references","title":"References","text":"<ol> <li>https://huggingface.co/blog/moe</li> </ol>"},{"location":"model_architecture_families/mixture_of_experts/","title":"Mixture of Models (MoE)","text":""},{"location":"model_architecture_families/mixture_of_experts/#1-overview","title":"1. Overview","text":"<p>Mixture of Experts (MoE) is an architectural paradigm that enables scaling model capacity to frontier levels while keeping per-token inference compute manageable. It allows a model to store far more knowledge than a dense model with similar inference cost, making it a key technique behind models such as GPT-4, Mixtral, and Grok.</p>"},{"location":"model_architecture_families/mixture_of_experts/#2-core-concept-and-intuition","title":"2. Core Concept and Intuition","text":"<p>In a standard dense Transformer, every parameter participates in processing every token.</p> <p>The problem with dense scaling</p> <ul> <li>Increasing parameters increases capacity</li> <li>But inference cost, latency, and memory usage scale linearly with model size</li> </ul> <p>The MoE solution</p> <p>MoE decouples model capacity from inference compute by activating only a small subset of parameters for each token.</p>"},{"location":"model_architecture_families/mixture_of_experts/#the-specialist-analogy","title":"The Specialist Analogy","text":"<p>Instead of one generalist handling all tasks, imagine a panel of specialists.</p> <ul> <li>A routing system decides which specialists should handle each input</li> <li>Only those specialists are consulted</li> </ul> <p>Key distinction:</p> <ul> <li>Total parameters represent the full knowledge capacity</li> <li>Active parameters determine inference cost for a given token</li> </ul>"},{"location":"model_architecture_families/mixture_of_experts/#3-architecture-the-sparse-transformer","title":"3. Architecture: The Sparse Transformer","text":"<p>An MoE model is identical to a standard Transformer except that the Feed-Forward Network (FFN) layers are replaced with MoE layers.</p>"},{"location":"model_architecture_families/mixture_of_experts/#components-of-an-moe-layer","title":"Components of an MoE Layer","text":"<ol> <li>Experts (\\(E_i\\)) </li> </ol> <p>A set of \\(N\\) independent FFNs, each with its own parameters.</p> <ol> <li>Router / Gating Network (\\(G\\)) </li> </ol> <p>A small learnable function that scores which experts should process a given token.</p>"},{"location":"model_architecture_families/mixture_of_experts/#routing-mechanism","title":"Routing Mechanism","text":"<p>For an input token representation \\(x\\), the output of an MoE layer is:</p> \\[ y = \\sum_{i=1}^{N} G(x)_i \\cdot E_i(x) \\] <p>In sparse MoE, a Top-k routing strategy is used:</p> <ul> <li>Only the top \\(k\\) experts receive non-zero weights</li> <li>All other experts are skipped entirely</li> <li>Typically \\(k = 1\\) or \\(k = 2\\)</li> </ul> <p>Only the selected experts are evaluated, making computation and gradient flow sparse.</p>"},{"location":"model_architecture_families/mixture_of_experts/#case-study-mixtral-8x7b","title":"Case Study: Mixtral 8x7B","text":"<ul> <li>Total experts: 8</li> <li>Routing: Top-2 per token</li> <li>Active parameters per token: ~13B</li> <li>Total parameters: ~47B</li> </ul> <p>The model exhibits capacity comparable to a ~50B dense model while running at the speed of a ~13B model.</p>"},{"location":"model_architecture_families/mixture_of_experts/#4-expert-capacity-and-token-dropping","title":"4. Expert Capacity and Token Dropping","text":"<p>Each expert has a fixed capacity, which limits how many tokens it can process in a single batch. This capacity is typically set as a multiple of the expected average load per expert.</p> <p>If too many tokens are routed to the same expert in a batch:</p> <ul> <li>Excess tokens may be dropped entirely</li> <li>Or processed with reduced routing weight, meaning their contribution to the expert\u2019s output is scaled down to maintain numerical and compute stability</li> <li>Or rerouted to fallback experts, depending on the implementation</li> </ul> <p>This mechanism prevents individual experts from becoming compute or memory bottlenecks but introduces a trade-off:</p> <ul> <li>Larger capacity improves training stability and model quality</li> <li>Smaller capacity improves efficiency but risks silent quality degradation due to dropped tokens</li> </ul> <p>Monitoring expert utilization and token dropping rates is therefore critical during training and debugging MoE models.</p>"},{"location":"model_architecture_families/mixture_of_experts/#why-reduce-routing-weight-even-when-the-expert-is-correct","title":"Why Reduce Routing Weight Even When the Expert Is Correct?","text":"<p>Routing decides which expert is best for a token. Capacity limits decide how much influence that token is allowed to have in a given batch.</p> <p>When an expert exceeds its capacity, all routed tokens are still correct assignments, but the system cannot afford to:</p> <ul> <li>Process unlimited tokens</li> <li>Accumulate unbounded gradients</li> <li>Let one expert dominate training</li> </ul> <p>Reducing the routing weight is a soft fallback:</p> <ul> <li>The token is still processed by the correct expert</li> <li>Its output and gradients are scaled down</li> <li>Compute and training stability are preserved</li> </ul> <p>The reduced weight does not indicate lower correctness. It limits influence to protect compute budgets and prevent expert collapse while retaining partial learning signal.</p>"},{"location":"model_architecture_families/mixture_of_experts/#5-training-dynamics-and-stability","title":"5. Training Dynamics and Stability","text":""},{"location":"model_architecture_families/mixture_of_experts/#benefits-of-moe-training","title":"Benefits of MoE Training","text":"<ul> <li>Compute efficiency: Lower validation loss for the same training FLOPs compared to dense models</li> <li>Knowledge scaling: Experts can store long-tail facts and rare patterns efficiently</li> <li>Faster convergence: Sparse FFNs reduce redundant computation</li> </ul>"},{"location":"model_architecture_families/mixture_of_experts/#mode-collapse-and-expert-imbalance","title":"Mode Collapse and Expert Imbalance","text":"<p>A common failure mode is expert collapse:</p> <ul> <li>Early-random advantages cause one expert to receive more tokens</li> <li>That expert improves faster due to more gradients</li> <li>Other experts receive fewer updates and remain undertrained</li> </ul>"},{"location":"model_architecture_families/mixture_of_experts/#auxiliary-losses-for-stability","title":"Auxiliary Losses for Stability","text":"<p>To prevent collapse, MoE training includes additional losses:</p> <ul> <li>Load Balancing Loss: Penalizes uneven token distribution across experts</li> <li>Z-Loss: Penalizes large router logits to improve numerical stability</li> </ul> <p>These losses are essential for maintaining expert diversity.</p>"},{"location":"model_architecture_families/mixture_of_experts/#5-emergent-expert-specialization","title":"5. Emergent Expert Specialization","text":"<p>Experts are not manually assigned domains.</p> <p>Specialization emerges implicitly from:</p> <ul> <li>Routing gradients</li> <li>Data distribution</li> <li>Load balancing constraints</li> </ul> <p>In practice, experts often specialize in:</p> <ul> <li>Syntax and formatting</li> <li>Punctuation and boilerplate</li> <li>Code versus natural language</li> <li>Long-context versus short-context tokens</li> </ul> <p>MoE does not guarantee clean semantic specialization such as math or biology experts.</p>"},{"location":"model_architecture_families/mixture_of_experts/#6-what-moe-improves-and-what-it-does-not","title":"6. What MoE Improves and What It Does Not","text":""},{"location":"model_architecture_families/mixture_of_experts/#moe-primarily-improves","title":"MoE Primarily Improves","text":"<ul> <li>Factual recall</li> <li>Coverage of rare or long-tail patterns</li> <li>Knowledge density per inference FLOP</li> </ul>"},{"location":"model_architecture_families/mixture_of_experts/#moe-does-not-automatically-improve","title":"MoE Does Not Automatically Improve","text":"<ul> <li>Multi-step reasoning</li> <li>Logical consistency</li> <li>Planning and abstraction</li> </ul> <p>Reasoning quality depends more on:</p> <ul> <li>Attention mechanisms</li> <li>Data quality</li> <li>Post-training alignment and RL</li> </ul>"},{"location":"model_architecture_families/mixture_of_experts/#7-inference-and-deployment-trade-offs","title":"7. Inference and Deployment Trade-offs","text":"Aspect Impact Throughput High, due to sparse computation Latency Low, driven by active parameter count VRAM Usage Very high, since all experts must be resident Communication High, requires all-to-all routing in distributed setups <p>MoE models are often memory-bandwidth bound, not compute-bound.</p>"},{"location":"model_architecture_families/mixture_of_experts/#8-training-cost-vs-inference-cost","title":"8. Training Cost vs Inference Cost","text":"<p>MoE reduces inference cost but increases training complexity:</p> <ul> <li>More communication overhead</li> <li>More fragile optimization</li> <li>Harder distributed orchestration</li> </ul> <p>MoE is most effective when:</p> <ul> <li>A model is trained once</li> <li>Served at massive scale</li> <li>Inference cost dominates total lifetime cost</li> </ul> <p>Dense models may be preferable for smaller-scale or latency-critical use cases.</p>"},{"location":"model_architecture_families/mixture_of_experts/#9-moe-in-the-scaling-toolbox","title":"9. MoE in the Scaling Toolbox","text":"Strategy Key Idea Trade-off Dense scaling Increase parameters Expensive inference MoE Sparse activation Memory and communication overhead Longer training More tokens per parameter Higher one-time cost Quantization Lower precision Potential accuracy loss <p>MoE is a powerful but specialized tool, not a universal solution.</p>"},{"location":"model_architecture_families/mixture_of_experts/#10-key-takeaways","title":"10. Key Takeaways","text":"<ul> <li>MoE decouples capacity from inference compute</li> <li>It is most effective for knowledge-heavy scaling</li> <li>Training is harder, inference is cheaper</li> <li>Many failures stem from routing imbalance and systems constraints</li> </ul> <p>MoE reflects a broader trend in modern LLMs: scaling is as much a systems problem as it is a modeling problem.</p>"},{"location":"model_architecture_families/mixture_of_experts/#11-some-questions","title":"11. Some Questions","text":"<p>Q: Does MoE reduce attention bottlenecks? A: No. MoE typically replaces FFN layers. Attention remains dense, so KV cache memory and attention compute are unchanged.</p> <p>Q: Why use Top-2 routing instead of Top-1? A: Top-2 provides smoother gradients and backup information flow, improving training stability.</p> <p>Q: What is the main bottleneck when serving MoE models? A: Memory bandwidth and communication, not raw FLOPs.</p>"},{"location":"model_architecture_families/mixture_of_experts/#references","title":"References","text":"<ol> <li>https://huggingface.co/blog/moe</li> </ol>"},{"location":"positional_encodings/ALiBi/","title":"ALiBi","text":"<p>Rotary Positional Embedding (RoPE) is the state-of-the-art method for encoding positional information in Transformers. It is the default choice for modern LLMs, including Llama 2/3, Mistral, and PaLM.</p>"},{"location":"positional_encodings/ALiBi/#1-the-core-concept","title":"1. The Core Concept","text":"<p>Traditional embeddings like Sinusoidal or Learned Absolute Embeddings are additive:</p> \\[ x_i = e_i + p_i \\] <p>RoPE is multiplicative. It treats the embedding vector as a set of complex numbers and rotates them in a high-dimensional space based on their position.</p>"},{"location":"positional_encodings/ALiBi/#why-rotate","title":"Why rotate?","text":"<p>The primary goal of RoPE is to ensure that the dot product between two tokens (Query and Key) depends only on their relative distance, not their absolute positions.</p> \\[ \\langle f_q(x_m, m), f_k(x_n, n) \\rangle = g(x_m, x_n, m - n) \\] <p>Where:</p> <ul> <li> <p>\\(x_m, x_n\\) : The original content embeddings of the tokens at positions \\(m\\) and \\(n\\). These vectors come from the token embedding layer or the previous Transformer block and do not yet include positional information.</p> </li> <li> <p>\\(m, n\\): The absolute token positions in the sequence. For example, \\(m = 5\\) and \\(n = 12\\).</p> </li> <li> <p>\\(f_q(x_m, m)\\): The position-aware Query representation at position \\(m\\).   This function applies the RoPE rotation to the query vector derived from \\(x_m\\).</p> </li> <li> <p>\\(f_k(x_n, n)\\): The position-aware Key representation at position \\(n\\).   This function applies the same RoPE rotation scheme to the key vector derived from \\(x_n\\).</p> </li> <li> <p>\\(\\langle \\cdot , \\cdot \\rangle\\): The dot product used in scaled dot-product attention to compute attention scores.</p> </li> <li> <p>\\(g(x_m, x_n, m - n)\\): A function whose output depends on the token contents and only the relative position \\((m - n)\\), not on the absolute positions themselves.</p> </li> </ul>"},{"location":"positional_encodings/ALiBi/#why-this-matters","title":"Why This Matters","text":"<p>This equation states that after applying RoPE:</p> <ul> <li>Absolute positions \\(m\\) and \\(n\\) do not independently affect the attention score.</li> <li>Only the relative distance \\((m - n)\\) influences how strongly two tokens attend to each other.</li> <li>This property enables strong length extrapolation, a natural locality bias, and robust long-context behavior.</li> <li>This allows the model to understand relationships between tokens regardless of where they appear in the sequence.</li> </ul> <p>In contrast, additive positional embeddings typically produce attention scores that depend on both absolute and relative positions, which limits generalization to longer sequences.</p>"},{"location":"positional_encodings/ALiBi/#2-the-algorithm","title":"2. The Algorithm","text":"<p>RoPE works by pairing elements of the \\(d\\)-dimensional embedding and applying a 2D rotation matrix to each pair.</p> <p>For a pair of dimensions \\([x^{(1)}, x^{(2)}]\\) at position \\(m\\), the transformation is:</p> \\[ \\begin{pmatrix} \\cos m\\theta &amp; -\\sin m\\theta \\\\ \\sin m\\theta &amp; \\cos m\\theta \\end{pmatrix} \\begin{pmatrix} x^{(1)} \\\\ x^{(2)} \\end{pmatrix} \\] <ul> <li> <p>The Angle (\\(\\theta\\)): The rotation frequency follows a geometric progression   $$   \\theta_i = 10000^{-2i/d}   $$   similar to sinusoidal embeddings.</p> </li> <li> <p>The Result: As position \\(m\\) increases, the vector rotates further. Because the rotation matrix is orthogonal, the vector norm is preserved while positional information is encoded.</p> </li> </ul>"},{"location":"positional_encodings/ALiBi/#3-where-exactly-is-rope-applied","title":"3. Where Exactly Is RoPE Applied?","text":"<p>RoPE is applied only to the Query and Key vectors in self-attention.</p> <ul> <li>Queries and Keys are rotated based on token position.</li> <li>Values are left unchanged.</li> </ul> <p>Reason:</p> <ul> <li>Attention scores are computed using the dot product \\(QK^\\top\\).</li> <li>RoPE ensures this dot product captures relative position.</li> <li>Applying RoPE to Values does not improve positional reasoning and can degrade performance.</li> </ul>"},{"location":"positional_encodings/ALiBi/#4-why-rope-works-for-dot-product-attention","title":"4. Why RoPE Works for Dot-Product Attention","text":"<p>The key mathematical insight behind RoPE is rotation composition.</p> <p>Consider the equation:</p> \\[ \\langle R(m)x, R(n)y \\rangle = \\langle x, R(n - m)y \\rangle \\] <p>Where:</p> <ul> <li> <p>\\(x, y\\): Content vectors representing token embeddings after linear projection into Query or Key space. These vectors do not contain positional information by themselves.</p> </li> <li> <p>\\(m, n\\): Absolute token positions in the sequence. For example, \\(m = 4\\) and \\(n = 10\\)</p> </li> <li> <p>\\(R(m)\\), \\(R(n)\\): Orthogonal rotation matrices parameterized by position.   Each matrix applies a set of 2D rotations across paired embedding dimensions, with rotation angles proportional to the position index.</p> </li> <li> <p>\\(\\langle \\cdot , \\cdot \\rangle\\): The standard dot product used in attention score computation.</p> </li> </ul>"},{"location":"positional_encodings/ALiBi/#step-by-step-intuition","title":"Step-by-Step Intuition","text":"<ol> <li> <p>Position Encoding via Rotation    Applying \\(R(m)\\) to \\(x\\) rotates the vector in embedding space by an amount determined by position \\(m\\).    Similarly, \\(R(n)\\) rotates \\(y\\) according to position \\(n\\).</p> </li> <li> <p>Orthogonality Property    Rotation matrices are orthogonal, meaning:    $$    R(m)^\\top = R(-m)    $$</p> </li> <li> <p>Rewriting the Dot Product    Using orthogonality:    $$    \\langle R(m)x, R(n)y \\rangle    =    \\langle x, R(m)^\\top R(n) y \\rangle    =    \\langle x, R(n - m) y \\rangle    $$</p> </li> <li> <p>Cancellation of Absolute Position    The absolute positions \\(m\\) and \\(n\\) collapse into a single relative offset \\((n - m)\\).</p> </li> </ol> <p>This identity captures the core mathematical reason why RoPE encodes relative position rather than absolute position. Below is a detailed explanation of each term and why the equation holds.</p> <p>Key implications:</p> <ul> <li>Absolute positions cancel out.</li> <li>Only the relative offset \\((n - m)\\) matters.</li> <li>This property aligns perfectly with dot-product attention.</li> <li>Tokens with the same relative spacing produce the same positional interaction, regardless of where they appear in the sequence.</li> </ul> <p>This is why RoPE integrates naturally into Transformer architectures.</p>"},{"location":"positional_encodings/ALiBi/#5-rope-and-multi-head-attention","title":"5. RoPE and Multi-Head Attention","text":"<p>In multi-head attention:</p> <ul> <li>RoPE is applied independently within each attention head.</li> <li>Each head operates on a lower-dimensional subspace and applies the same frequency schedule.</li> </ul> <p>As a result:</p> <ul> <li>Low-frequency dimensions capture long-range dependencies.</li> <li>High-frequency dimensions focus on local structure.</li> </ul> <p>This creates a multi-scale positional representation across heads.</p>"},{"location":"positional_encodings/ALiBi/#6-comparison-why-rope-won","title":"6. Comparison: Why RoPE Won","text":"Feature Absolute (Sinusoidal) RoPE Operation Addition Rotation (Multiplication) Relative Distance Not explicit Naturally captured via dot product Extrapolation Weak for long contexts Strong with scaling Decay No natural decay Distance-based interaction decay Implementation Simple Moderate complexity"},{"location":"positional_encodings/ALiBi/#7-small-intuitive-example","title":"7. Small Intuitive Example","text":"<p>Consider a single 2D vector:</p> \\[ x = [1, 0] \\] <p>Assume one frequency \\(\\theta = 0.1\\).</p>"},{"location":"positional_encodings/ALiBi/#position-1","title":"Position 1","text":"<p>Rotation angle = \\(0.1\\)</p> \\[ R(0.1)x = [\\cos 0.1, \\sin 0.1] \\approx [0.995, 0.100] \\]"},{"location":"positional_encodings/ALiBi/#position-3","title":"Position 3","text":"<p>Rotation angle = \\(0.3\\)</p> \\[ R(0.3)x = [\\cos 0.3, \\sin 0.3] \\approx [0.955, 0.296] \\] <p>Dot product between the two:</p> \\[ \\langle R(0.1)x, R(0.3)x \\rangle = \\cos(0.2) \\] <p>Observation: - The dot product depends only on the positional difference \\((3 - 1)\\). - Vector magnitude is preserved. - Relative distance is directly encoded into attention.</p>"},{"location":"positional_encodings/ALiBi/#8-interview-deep-dive-topics","title":"8. Interview Deep Dive Topics","text":""},{"location":"positional_encodings/ALiBi/#a-context-window-extension-rope-scaling","title":"A. Context Window Extension (RoPE Scaling)","text":"<p>Question: If a model is trained on 4k context, how can it be extended to 128k?</p> <ul> <li> <p>Linear Interpolation: Scale positions as   $$   m \\rightarrow m \\cdot \\frac{L}{L'}   $$   to avoid unseen large angles.</p> </li> <li> <p>NTK-aware Scaling: Scale different frequencies differently to preserve high-frequency information for nearby tokens.</p> </li> </ul>"},{"location":"positional_encodings/ALiBi/#b-long-term-decay-property","title":"B. Long-Term Decay Property","text":"<p>RoPE introduces a natural decay in interaction strength as \\(|m - n|\\) increases. This provides a locality bias without enforcing a hard attention window, aligning well with natural language structure.</p>"},{"location":"positional_encodings/ALiBi/#c-rope-vs-alibi","title":"C. RoPE vs ALiBi","text":"<ul> <li>ALiBi adds a linear bias to attention scores based on distance.</li> <li>RoPE encodes position directly into representations, allowing richer and more flexible positional reasoning.</li> </ul>"},{"location":"positional_encodings/ALiBi/#9-practical-limitations-and-caveats","title":"9. Practical Limitations and Caveats","text":"<ul> <li>RoPE assumes evenly spaced token positions.</li> <li>Extreme extrapolation without scaling can cause numerical instability.</li> <li>Relative position is encoded strongly, but absolute position is implicit, which may matter for structured tasks.</li> </ul>"},{"location":"positional_encodings/RoPE/","title":"RoPE","text":"<p>Rotary Positional Embedding (RoPE) is the state-of-the-art method for encoding positional information in Transformers. It is the default choice for modern LLMs, including Llama 2/3, Mistral, and PaLM.</p>"},{"location":"positional_encodings/RoPE/#1-the-core-concept","title":"1. The Core Concept","text":"<p>Traditional embeddings like Sinusoidal or Learned Absolute Embeddings are additive:</p> \\[ x_i = e_i + p_i \\] <p>RoPE is multiplicative. It treats the embedding vector as a set of complex numbers and rotates them in a high-dimensional space based on their position.</p>"},{"location":"positional_encodings/RoPE/#why-rotate","title":"Why rotate?","text":"<p>The primary goal of RoPE is to ensure that the dot product between two tokens (Query and Key) depends only on their relative distance, not their absolute positions.</p> \\[ \\langle f_q(x_m, m), f_k(x_n, n) \\rangle = g(x_m, x_n, m - n) \\] <p>Where:</p> <ul> <li> <p>\\(x_m, x_n\\) : The original content embeddings of the tokens at positions \\(m\\) and \\(n\\). These vectors come from the token embedding layer or the previous Transformer block and do not yet include positional information.</p> </li> <li> <p>\\(m, n\\): The absolute token positions in the sequence. For example, \\(m = 5\\) and \\(n = 12\\).</p> </li> <li> <p>\\(f_q(x_m, m)\\): The position-aware Query representation at position \\(m\\).   This function applies the RoPE rotation to the query vector derived from \\(x_m\\).</p> </li> <li> <p>\\(f_k(x_n, n)\\): The position-aware Key representation at position \\(n\\).   This function applies the same RoPE rotation scheme to the key vector derived from \\(x_n\\).</p> </li> <li> <p>\\(\\langle \\cdot , \\cdot \\rangle\\): The dot product used in scaled dot-product attention to compute attention scores.</p> </li> <li> <p>\\(g(x_m, x_n, m - n)\\): A function whose output depends on the token contents and only the relative position \\((m - n)\\), not on the absolute positions themselves.</p> </li> </ul>"},{"location":"positional_encodings/RoPE/#why-this-matters","title":"Why This Matters","text":"<p>This equation states that after applying RoPE:</p> <ul> <li>Absolute positions \\(m\\) and \\(n\\) do not independently affect the attention score.</li> <li>Only the relative distance \\((m - n)\\) influences how strongly two tokens attend to each other.</li> <li>This property enables strong length extrapolation, a natural locality bias, and robust long-context behavior.</li> <li>This allows the model to understand relationships between tokens regardless of where they appear in the sequence.</li> </ul> <p>In contrast, additive positional embeddings typically produce attention scores that depend on both absolute and relative positions, which limits generalization to longer sequences.</p>"},{"location":"positional_encodings/RoPE/#2-the-algorithm","title":"2. The Algorithm","text":"<p>RoPE works by pairing elements of the \\(d\\)-dimensional embedding and applying a 2D rotation matrix to each pair.</p> <p>For a pair of dimensions \\([x^{(1)}, x^{(2)}]\\) at position \\(m\\), the transformation is:</p> \\[ \\begin{pmatrix} \\cos m\\theta &amp; -\\sin m\\theta \\\\ \\sin m\\theta &amp; \\cos m\\theta \\end{pmatrix} \\begin{pmatrix} x^{(1)} \\\\ x^{(2)} \\end{pmatrix} \\] <ul> <li> <p>The Angle (\\(\\theta\\)): The rotation frequency follows a geometric progression   $$   \\theta_i = 10000^{-2i/d}   $$   similar to sinusoidal embeddings.</p> </li> <li> <p>The Result: As position \\(m\\) increases, the vector rotates further. Because the rotation matrix is orthogonal, the vector norm is preserved while positional information is encoded.</p> </li> </ul>"},{"location":"positional_encodings/RoPE/#3-where-exactly-is-rope-applied","title":"3. Where Exactly Is RoPE Applied?","text":"<p>RoPE is applied only to the Query and Key vectors in self-attention.</p> <ul> <li>Queries and Keys are rotated based on token position.</li> <li>Values are left unchanged.</li> </ul> <p>Reason:</p> <ul> <li>Attention scores are computed using the dot product \\(QK^\\top\\).</li> <li>RoPE ensures this dot product captures relative position.</li> <li>Applying RoPE to Values does not improve positional reasoning and can degrade performance.</li> </ul>"},{"location":"positional_encodings/RoPE/#4-why-rope-works-for-dot-product-attention","title":"4. Why RoPE Works for Dot-Product Attention","text":"<p>The key mathematical insight behind RoPE is rotation composition.</p> <p>Consider the equation:</p> \\[ \\langle R(m)x, R(n)y \\rangle = \\langle x, R(n - m)y \\rangle \\] <p>Where:</p> <ul> <li> <p>\\(x, y\\): Content vectors representing token embeddings after linear projection into Query or Key space. These vectors do not contain positional information by themselves.</p> </li> <li> <p>\\(m, n\\): Absolute token positions in the sequence. For example, \\(m = 4\\) and \\(n = 10\\)</p> </li> <li> <p>\\(R(m)\\), \\(R(n)\\): Orthogonal rotation matrices parameterized by position.   Each matrix applies a set of 2D rotations across paired embedding dimensions, with rotation angles proportional to the position index.</p> </li> <li> <p>\\(\\langle \\cdot , \\cdot \\rangle\\): The standard dot product used in attention score computation.</p> </li> </ul>"},{"location":"positional_encodings/RoPE/#step-by-step-intuition","title":"Step-by-Step Intuition","text":"<ol> <li> <p>Position Encoding via Rotation    Applying \\(R(m)\\) to \\(x\\) rotates the vector in embedding space by an amount determined by position \\(m\\).    Similarly, \\(R(n)\\) rotates \\(y\\) according to position \\(n\\).</p> </li> <li> <p>Orthogonality Property    Rotation matrices are orthogonal, meaning:    $$    R(m)^\\top = R(-m)    $$</p> </li> <li> <p>Rewriting the Dot Product    Using orthogonality:    $$    \\langle R(m)x, R(n)y \\rangle    =    \\langle x, R(m)^\\top R(n) y \\rangle    =    \\langle x, R(n - m) y \\rangle    $$</p> </li> <li> <p>Cancellation of Absolute Position    The absolute positions \\(m\\) and \\(n\\) collapse into a single relative offset \\((n - m)\\).</p> </li> </ol> <p>This identity captures the core mathematical reason why RoPE encodes relative position rather than absolute position. Below is a detailed explanation of each term and why the equation holds.</p> <p>Key implications:</p> <ul> <li>Absolute positions cancel out.</li> <li>Only the relative offset \\((n - m)\\) matters.</li> <li>This property aligns perfectly with dot-product attention.</li> <li>Tokens with the same relative spacing produce the same positional interaction, regardless of where they appear in the sequence.</li> </ul> <p>This is why RoPE integrates naturally into Transformer architectures.</p>"},{"location":"positional_encodings/RoPE/#5-rope-and-multi-head-attention","title":"5. RoPE and Multi-Head Attention","text":"<p>In multi-head attention:</p> <ul> <li>RoPE is applied independently within each attention head.</li> <li>Each head operates on a lower-dimensional subspace and applies the same frequency schedule.</li> </ul> <p>As a result:</p> <ul> <li>Low-frequency dimensions capture long-range dependencies.</li> <li>High-frequency dimensions focus on local structure.</li> </ul> <p>This creates a multi-scale positional representation across heads.</p>"},{"location":"positional_encodings/RoPE/#6-comparison-why-rope-won","title":"6. Comparison: Why RoPE Won","text":"Feature Absolute (Sinusoidal) RoPE Operation Addition Rotation (Multiplication) Relative Distance Not explicit Naturally captured via dot product Extrapolation Weak for long contexts Strong with scaling Decay No natural decay Distance-based interaction decay Implementation Simple Moderate complexity"},{"location":"positional_encodings/RoPE/#7-small-intuitive-example","title":"7. Small Intuitive Example","text":"<p>Consider a single 2D vector:</p> \\[ x = [1, 0] \\] <p>Assume one frequency \\(\\theta = 0.1\\).</p>"},{"location":"positional_encodings/RoPE/#position-1","title":"Position 1","text":"<p>Rotation angle = \\(0.1\\)</p> \\[ R(0.1)x = [\\cos 0.1, \\sin 0.1] \\approx [0.995, 0.100] \\]"},{"location":"positional_encodings/RoPE/#position-3","title":"Position 3","text":"<p>Rotation angle = \\(0.3\\)</p> \\[ R(0.3)x = [\\cos 0.3, \\sin 0.3] \\approx [0.955, 0.296] \\] <p>Dot product between the two:</p> \\[ \\langle R(0.1)x, R(0.3)x \\rangle = \\cos(0.2) \\] <p>Observation: - The dot product depends only on the positional difference \\((3 - 1)\\). - Vector magnitude is preserved. - Relative distance is directly encoded into attention.</p>"},{"location":"positional_encodings/RoPE/#8-interview-deep-dive-topics","title":"8. Interview Deep Dive Topics","text":""},{"location":"positional_encodings/RoPE/#a-context-window-extension-rope-scaling","title":"A. Context Window Extension (RoPE Scaling)","text":"<p>Question: If a model is trained on 4k context, how can it be extended to 128k?</p> <ul> <li> <p>Linear Interpolation: Scale positions as   $$   m \\rightarrow m \\cdot \\frac{L}{L'}   $$   to avoid unseen large angles.</p> </li> <li> <p>NTK-aware Scaling: Scale different frequencies differently to preserve high-frequency information for nearby tokens.</p> </li> </ul>"},{"location":"positional_encodings/RoPE/#b-long-term-decay-property","title":"B. Long-Term Decay Property","text":"<p>RoPE introduces a natural decay in interaction strength as \\(|m - n|\\) increases. This provides a locality bias without enforcing a hard attention window, aligning well with natural language structure.</p>"},{"location":"positional_encodings/RoPE/#c-rope-vs-alibi","title":"C. RoPE vs ALiBi","text":"<ul> <li>ALiBi adds a linear bias to attention scores based on distance.</li> <li>RoPE encodes position directly into representations, allowing richer and more flexible positional reasoning.</li> </ul>"},{"location":"positional_encodings/RoPE/#9-practical-limitations-and-caveats","title":"9. Practical Limitations and Caveats","text":"<ul> <li>RoPE assumes evenly spaced token positions.</li> <li>Extreme extrapolation without scaling can cause numerical instability.</li> <li>Relative position is encoded strongly, but absolute position is implicit, which may matter for structured tasks.</li> </ul>"},{"location":"real_world_model_families/claude/","title":"Claude","text":""},{"location":"real_world_model_families/claude/#1-the-claude-model-family-current-landscape","title":"1. The Claude Model Family (Current Landscape)","text":"<p>Anthropic uses a \"Tier\" system (Haiku, Sonnet, Opus) across generations. As of 2026, the 4.5 Generation is the state-of-the-art.</p> Model Generation Tier Primary Use Case Claude 4.5 Opus Frontier System 2 Extreme reasoning, scientific discovery, autonomous agents. Claude 4.5 Sonnet Mid-tier Hybrid Software engineering (Claude Code), enterprise workflows. Claude 4.5 Haiku Lightweight System 1 High-speed API tasks, sub-second latency requirements. <p>Note: Claude 3.7 Sonnet was the landmark \"Hybrid\" model that first introduced a toggle between instant (System 1) and extended thinking (System 2) modes.</p>"},{"location":"real_world_model_families/claude/#2-model-tiers-their-cognitive-roles","title":"2. Model Tiers &amp; Their Cognitive Roles","text":"<p>Anthropic maintains three tiers (Haiku, Sonnet, Opus). In the 2025\u20132026 era, their roles have shifted toward specific agentic behaviors.</p>"},{"location":"real_world_model_families/claude/#tier-1-haiku-the-system-1-specialist","title":"Tier 1: Haiku (The \"System 1\" Specialist)","text":"<ul> <li>Mode: Primarily System 1.</li> <li>Role: High-speed, high-volume tasks.</li> <li>Context: Used as the \"Router\" or \"Small Brain\" in agentic workflows to handle simple classification or fast sub-tasks.</li> </ul>"},{"location":"real_world_model_families/claude/#tier-2-sonnet-the-hybridagentic-workhorse","title":"Tier 2: Sonnet (The \"Hybrid/Agentic\" Workhorse)","text":"<ul> <li>Mode: Hybrid.</li> <li>Role: This is Anthropic's flagship tier for Agents. </li> <li>Key Feature: Computer Use. Sonnet 3.5 and 4.5 are specifically tuned to use tools, navigate UIs, and write code via the <code>Claude Code</code> CLI.</li> </ul>"},{"location":"real_world_model_families/claude/#tier-3-opus-the-system-2-frontier","title":"Tier 3: Opus (The \"System 2\" Frontier)","text":"<ul> <li>Mode: Deep System 2.</li> <li>Role: Reserved for tasks where accuracy is more important than speed (e.g., drug discovery, complex legal analysis). </li> <li>Context: Inference-Time Scaling Laws - the idea that more \"thinking time\" leads to higher IQ.</li> </ul>"},{"location":"real_world_model_families/claude/#3-the-agentic-evolution","title":"3. The \"Agentic\" Evolution","text":"<p>While \"System 2\" refers to how the model thinks, \"Agentic\" refers to what the model can do.</p> <ul> <li>Claude 3.5: Introduced the \"Agentic Framework\" (Computer Use). It was a System 1 model trying to do System 2 tasks via tool-calling.</li> <li>Claude 3.7: Improved Agentic reliability by allowing the model to \"think\" (System 2) before it clicks a button or writes a line of code.</li> <li>Claude 4.5: Fully Agentic-Native. The model doesn't just call tools; it plans the entire multi-step trajectory using hidden thinking tokens before execution.<ul> <li>Claude Code: A specialized agentic interface that uses these capabilities to perform autonomous software engineering tasks (e.g., \"Fix the bug in the auth flow and run the tests\").</li> </ul> </li> </ul>"},{"location":"real_world_model_families/claude/#4-core-differentiator-constitutional-ai-cai","title":"4. Core Differentiator: Constitutional AI (CAI)","text":"<p>Anthropic\u2019s defining strategy is Constitutional AI, which enables RLAIF (Reinforcement Learning from AI Feedback).</p>"},{"location":"real_world_model_families/claude/#the-training-pipeline","title":"The Training Pipeline","text":"<ol> <li>The Constitution: A set of rules (e.g., \"Be helpful, honest, and harmless\") used as a rubric.</li> <li>Critique and Revision: The model generates an initial response, then critiques itself based on the Constitution, and finally rewrites it.</li> <li>Preference Model (RLAIF): A separate AI model evaluates thousands of these pairs to create a \"Reward Model.\" </li> <li>Reinforcement Learning: The final model is fine-tuned to maximize the rewards defined by that AI-driven preference model.</li> </ol> <p>Insight: This reduces Sycophancy (the tendency of models to agree with users just to be \"likable\") because the model is optimized for a fixed Constitution rather than fickle human ratings.</p>"},{"location":"real_world_model_families/claude/#5-system-1-vs-system-2-reasoning","title":"5. System 1 vs. System 2 Reasoning","text":"<p>Anthropic has moved beyond \"prompting for step-by-step\" into true Inference-Time Scaling.</p>"},{"location":"real_world_model_families/claude/#system-1-fastintuitive","title":"System 1: Fast/Intuitive","text":"<ul> <li>Mechanism: Standard next-token prediction.</li> <li>Latency: Instant.</li> <li>Use Case: Brainstorming, simple chat, data extraction.</li> </ul>"},{"location":"real_world_model_families/claude/#system-2-deliberateextended-thinking","title":"System 2: Deliberate/Extended Thinking","text":"<ul> <li>Mechanism: The model generates Thinking Tokens (visible or hidden) before producing the final answer. This allows the model to \"plan\" and \"search\" through different solution paths.</li> <li>Scaling Law: Performance improves as you increase the Thinking Budget (more compute at inference = higher intelligence).</li> <li>Models: Claude 3.7 Sonnet and Claude 4.5 Opus.</li> </ul>"},{"location":"real_world_model_families/claude/#6-summary-cheat-sheet-for-interviews","title":"6. Summary Cheat Sheet for Interviews","text":"Feature Technical Detail to Mention Alignment Constitutional AI / RLAIF (Scalable oversight). Reasoning Inference-time Scaling (System 2 thinking tokens). Architecture Likely Mixture of Experts (MoE) to balance speed and capacity. Agentic Computer Use API (Screenshots to x,y coordinate actions). Safety Focused on \"Red Teaming\" and \"ASL\" (AI Safety Levels)."},{"location":"real_world_model_families/dense_vs_MoE_comparison/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"real_world_model_families/dense_vs_MoE_comparison/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"real_world_model_families/dense_vs_MoE_comparison/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"real_world_model_families/gemini/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"real_world_model_families/gemini/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"real_world_model_families/gemini/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"real_world_model_families/general_comparison/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"real_world_model_families/general_comparison/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"real_world_model_families/general_comparison/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"real_world_model_families/llama/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"real_world_model_families/llama/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"real_world_model_families/llama/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"real_world_model_families/mistral/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"real_world_model_families/mistral/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"real_world_model_families/mistral/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"tokenization/byte_pair_encoding/","title":"Byte Pair Encoding","text":""},{"location":"tokenization/byte_pair_encoding/#1-overview","title":"1. Overview","text":"<p>Byte Pair Encoding (BPE) and its modern variants are the dominant subword tokenization methods used in today\u2019s Large Language Models (LLMs), including GPT-3, GPT-4, and LLaMA. BPE strikes a practical balance between word-level and character-level tokenization, enabling scalable training while avoiding out-of-vocabulary failures.</p> <p>BPE addresses the out-of-vocabulary (OOV) problem by decomposing text into frequently occurring subword units instead of relying on a fixed word vocabulary. Any unseen word can still be represented as a sequence of known subcomponents.</p> <p>This makes BPE robust, flexible, and suitable for web-scale corpora with noisy and evolving language.</p>"},{"location":"tokenization/byte_pair_encoding/#2-how-bpe-works-the-algorithm","title":"2. How BPE Works (The Algorithm)","text":"<ol> <li> <p>Initialize    Start with a vocabulary of base symbols, either characters or UTF-8 bytes.</p> </li> <li> <p>Frequency Analysis    Count all adjacent token pairs in the training corpus.</p> </li> <li> <p>Merge    Replace the most frequent adjacent pair with a new token.</p> </li> <li> <p>Iterate    Repeat until a predefined vocabulary size is reached.</p> </li> </ol>"},{"location":"tokenization/byte_pair_encoding/#21-step-by-step-example","title":"2.1 Step-by-Step Example","text":"<p>Consider a tiny corpus: [low, lower, newest, widest]</p>"},{"location":"tokenization/byte_pair_encoding/#step-1-initialize","title":"Step 1: Initialize","text":"<p>Start with characters as the base vocabulary and mark word boundaries:</p> <p>l o w _  l o w e r _  n e w e s t _  w i d e s t _ </p> <p>Initial tokens are individual characters.</p>"},{"location":"tokenization/byte_pair_encoding/#step-2-frequency-analysis","title":"Step 2: Frequency Analysis","text":"<p>Count all adjacent token pairs across the corpus.</p> <p>Some frequent pairs:</p> <ul> <li><code>l o</code></li> <li><code>o w</code></li> <li><code>e s</code></li> <li><code>s t</code></li> </ul> <p>Assume <code>o w</code> is the most frequent pair.</p>"},{"location":"tokenization/byte_pair_encoding/#step-3-merge","title":"Step 3: Merge","text":"<p>Merge <code>o w \u2192 ow</code> everywhere:</p> <p>l ow _  l ow e r _  n e w e s t _  w i d e s t _ </p> <p>Vocabulary now includes <code>ow</code>.</p>"},{"location":"tokenization/byte_pair_encoding/#step-4-iterate","title":"Step 4: Iterate","text":"<p>Repeat frequency analysis and merging.</p> <p>Next merges might be:</p> <ul> <li><code>l ow \u2192 low</code></li> <li><code>e s \u2192 es</code></li> <li><code>es t \u2192 est</code></li> </ul> <p>Eventually the corpus may look like:</p> <p>low _  lower _  new est _  wid est _ </p> <p>And the vocabulary contains: [l, o, w, e, r, n, i, d, t, low, est, wid, lower, _]</p> <p>The process stops once the target vocabulary size is reached.</p>"},{"location":"tokenization/byte_pair_encoding/#22-vocabulary-size","title":"2.2 Vocabulary Size","text":"<p>The final vocabulary size is:</p> \\[ V = S + N_{\\text{merges}} \\] <p>Where: - \\(S\\) is the number of base symbols - \\(N_{\\text{merges}}\\) is the number of merge operations</p> <p>Each merge permanently adds one new token.</p>"},{"location":"tokenization/byte_pair_encoding/#23-key-properties","title":"2.3 Key Properties","text":"<ul> <li>Greedy: Always merges the most frequent pair</li> <li>Deterministic: Same data and merges produce the same vocabulary</li> <li>Irreversible: Bad early merges cannot be undone</li> </ul>"},{"location":"tokenization/byte_pair_encoding/#24-where-bpe-shines","title":"2.4 Where BPE Shines","text":""},{"location":"tokenization/byte_pair_encoding/#1-common-words-and-morphemes","title":"1. Common Words and Morphemes","text":"<p>Frequently occurring patterns like:</p> <ul> <li><code>ing</code>, <code>tion</code>, <code>http</code>, <code>://</code> become single tokens, reducing sequence length and improving efficiency.</li> </ul>"},{"location":"tokenization/byte_pair_encoding/#2-open-vocabulary","title":"2. Open Vocabulary","text":"<p>Any new word can be represented as subwords: </p> <p><code>unhappiness \u2192 un + happi + ness</code></p> <p>No <code>[UNK]</code> tokens are required.</p>"},{"location":"tokenization/byte_pair_encoding/#3-web-scale-text","title":"3. Web-Scale Text","text":"<p>BPE handles:</p> <ul> <li>URLs</li> <li>Code</li> <li>Typos</li> <li>Mixed-language text</li> </ul> <p>very well, especially in byte-level variants.</p>"},{"location":"tokenization/byte_pair_encoding/#25-where-bpe-fails-or-struggles","title":"2.5 Where BPE Fails or Struggles","text":""},{"location":"tokenization/byte_pair_encoding/#1-rare-words-and-low-resource-languages","title":"1. Rare Words and Low-Resource Languages","text":"<p>Languages with rich morphology or limited data are often split into many tokens, making them:</p> <ul> <li>More expensive to process</li> <li>Harder to learn effectively</li> </ul>"},{"location":"tokenization/byte_pair_encoding/#2-numbers-and-arithmetic","title":"2. Numbers and Arithmetic","text":"<p>BPE tokenizes numbers inconsistently:</p> <p><code>1000 \u2192 [1000]</code> <code>10001 \u2192 [100, 01]</code></p> <p>This breaks digit-level reasoning and arithmetic.</p>"},{"location":"tokenization/byte_pair_encoding/#3-spelling-and-character-level-tasks","title":"3. Spelling and Character-Level Tasks","text":"<p>If a word becomes a single token: <code>strawberry \u2192 [strawberry]</code></p> <p>The model cannot directly reason about individual letters.</p>"},{"location":"tokenization/byte_pair_encoding/#4-early-merge-bias","title":"4. Early Merge Bias","text":"<p>Because merges are greedy:</p> <ul> <li>Early frequent patterns dominate the vocabulary</li> <li>Suboptimal merges persist forever</li> <li>Later data distributions cannot correct them</li> </ul>"},{"location":"tokenization/byte_pair_encoding/#5-sensitivity-to-formatting","title":"5. Sensitivity to Formatting","text":"<p>Whitespace, casing, and punctuation affect token boundaries: <code>\"hello\" \u2260 \" hello\"</code></p> <p>Prompt formatting can significantly change tokenization.</p>"},{"location":"tokenization/byte_pair_encoding/#3-tokenization-strategies-compared","title":"3. Tokenization Strategies Compared","text":"Strategy Pros Cons Word-level High semantic meaning Huge vocabulary, OOV issues Character-level No OOV, small vocabulary Long sequences, weak semantics Subword (BPE) Balanced efficiency and flexibility Linguistically unintuitive splits"},{"location":"tokenization/byte_pair_encoding/#4-why-bpe-works-well-for-llms","title":"4. Why BPE Works Well for LLMs","text":"<ul> <li> <p>Token Efficiency   Frequent strings like <code>the</code>, <code>ing</code>, or <code>http</code> become single tokens, reducing sequence length.</p> </li> <li> <p>Statistical Morphology   Related words such as <code>play</code>, <code>playing</code>, and <code>played</code> often share subword units.   This is an emergent frequency effect, not explicit linguistic understanding.</p> </li> <li> <p>Byte-level Robustness   Byte-level BPE operates on UTF-8 bytes, guaranteeing that any string can be tokenized without an <code>[UNK]</code> token.</p> </li> </ul>"},{"location":"tokenization/byte_pair_encoding/#5-bpe-variants-used-in-practice","title":"5. BPE Variants Used in Practice","text":"<p>Modern LLMs rarely use vanilla BPE.</p> <ul> <li> <p>Byte-level BPE (GPT-2, GPT-4)   Uses bytes as base symbols, eliminating unknown tokens.</p> </li> <li> <p>SentencePiece BPE (LLaMA)   Avoids pre-tokenization and treats whitespace as a normal symbol.</p> </li> <li> <p>Unigram Language Model (SentencePiece)   Uses a probabilistic model over subwords instead of greedy merges, allowing multiple valid tokenizations.</p> </li> </ul> <p>Key distinction:</p> <ul> <li>BPE is deterministic and greedy</li> <li>Unigram LM is probabilistic and more flexible</li> </ul>"},{"location":"tokenization/byte_pair_encoding/#6-pre-tokenization-and-whitespace-effects","title":"6. Pre-tokenization and Whitespace Effects","text":"<p>Tokenization is sensitive to whitespace and formatting.</p> <ul> <li><code>\"hello\"</code> and <code>\" hello\"</code> may map to different tokens</li> <li>Leading spaces often act as token boundaries</li> <li>Prompt formatting directly influences tokenization</li> </ul> <p>These effects explain why small formatting changes can significantly alter model behavior.</p>"},{"location":"tokenization/byte_pair_encoding/#7-tokenization-paradoxes-interview-level-insights","title":"7. Tokenization Paradoxes (Interview-Level Insights)","text":""},{"location":"tokenization/byte_pair_encoding/#the-spelling-paradox","title":"The Spelling Paradox","text":"<p>LLMs struggle to count letters in words. - Reason: words often appear as single tokens, hiding character-level structure.</p>"},{"location":"tokenization/byte_pair_encoding/#the-arithmetic-paradox","title":"The Arithmetic Paradox","text":"<p>LLMs fail at digit-wise arithmetic on large numbers. - Reason: numbers are split inconsistently across tokens.</p>"},{"location":"tokenization/byte_pair_encoding/#the-case-sensitivity-trap","title":"The Case Sensitivity Trap","text":"<p><code>Hello</code>, <code>hello</code>, and <code>HELLO</code> are distinct tokens. - Effect: the model must learn redundant representations.</p>"},{"location":"tokenization/byte_pair_encoding/#8-pros-and-cons-of-bpe","title":"8. Pros and Cons of BPE","text":""},{"location":"tokenization/byte_pair_encoding/#pros","title":"Pros","text":"<ul> <li>Adaptive to training data</li> <li>No OOV failures</li> <li>Balanced embedding matrix size (typically 32k\u2013100k tokens)</li> </ul>"},{"location":"tokenization/byte_pair_encoding/#cons","title":"Cons","text":"<ul> <li>Greedy and irreversible merges</li> <li>English-centric bias</li> <li>Poor handling of low-resource languages</li> <li>Weak inductive bias for spelling and arithmetic</li> </ul>"},{"location":"tokenization/byte_pair_encoding/#takeaway","title":"Takeaway","text":"<p>BPE is best understood as a compression-driven algorithm, not a linguistic one. It excels at efficiency and scalability but introduces structural biases that directly affect reasoning, arithmetic, and multilingual performance.</p> <p>Many LLM failure modes are rooted in tokenization choices rather than model architecture.</p>"},{"location":"tokenization/byte_pair_encoding/#9-tokenization-is-part-of-the-model","title":"9. Tokenization Is Part of the Model","text":"<p>Tokenization defines:</p> <ul> <li>The atomic prediction units</li> <li>The embedding space structure</li> <li>What patterns are easy or hard to learn</li> </ul> <p>Changing tokenization usually requires:</p> <ul> <li>Relearning embeddings</li> <li>Often retraining the entire model</li> </ul> <p>Tokenization errors cannot be fixed by fine-tuning alone.</p>"},{"location":"tokenization/byte_pair_encoding/#10-tokenization-and-cost","title":"10. Tokenization and Cost","text":"<p>Tokenization affects:</p> <ul> <li>Context length utilization</li> <li>Inference latency</li> <li>Serving cost</li> </ul> <p>Languages or domains that produce more tokens:</p> <ul> <li>Consume context faster</li> <li>Cost more to serve</li> <li>Often exhibit lower effective performance</li> </ul> <p>This creates fairness and efficiency challenges in multilingual models.</p>"},{"location":"tokenization/byte_pair_encoding/#11-beyond-bpe-emerging-alternatives","title":"11. Beyond BPE: Emerging Alternatives","text":"<p>Active research explores:</p> <ul> <li>Character-aware transformers</li> <li>Byte-level models without merges</li> <li>Token-free architectures</li> </ul> <p>Motivation:</p> <ul> <li>Reduce tokenization artifacts</li> <li>Improve multilingual equity</li> <li>Improve spelling and arithmetic</li> </ul> <p>Trade-off:</p> <ul> <li>Higher compute cost</li> <li>Slower convergence</li> </ul> <p>As of 2025, BPE-based methods remain dominant due to their efficiency and scalability.</p>"},{"location":"tokenization/byte_pair_encoding/#12-interview-faq","title":"12. Interview FAQ","text":"<p>Q: How is BPE different from WordPiece? A: BPE merges based on raw frequency, while WordPiece merges to maximize likelihood under a language model.</p> <p>Q: Why use byte-level BPE? A: It guarantees full coverage with a base vocabulary of 256 bytes, eliminating unknown tokens.</p> <p>Q: Can tokenization be changed after training? A: Not easily. Tokenization changes typically require retraining embeddings and often the full model.</p>"},{"location":"tokenization/byte_pair_encoding/#13-conceptual-implementation-snippet","title":"13. Conceptual Implementation Snippet","text":"<pre><code>def get_stats(ids):\n    counts = {}\n    for pair in zip(ids, ids[1:]):\n        counts[pair] = counts.get(pair, 0) + 1\n    return counts\n\ndef merge(ids, pair, idx):\n    new_ids = []\n    i = 0\n    while i &lt; len(ids):\n        if i &lt; len(ids) - 1 and (ids[i], ids[i+1]) == pair:\n            new_ids.append(idx)\n            i += 2\n        else:\n            new_ids.append(ids[i])\n            i += 1\n    return new_ids\n</code></pre>"},{"location":"tokenization/sentence_piece_unigram/","title":"Sentence Piece Unigram","text":"<p>The Unigram Language Model is a probabilistic, top-down subword tokenization algorithm. While often associated with the SentencePiece library (the framework for lossless, language-agnostic tokenization), Unigram itself is a distinct algorithm used by models like T5, XLNet, ALBERT, and many modern multilingual LLMs.</p>"},{"location":"tokenization/sentence_piece_unigram/#1-the-probabilistic-framework","title":"1. The Probabilistic Framework","text":"<p>Unlike BPE, which is a greedy heuristic, Unigram treats tokenization as a statistical inference problem. It assumes that a sequence is generated by independently sampling tokens from a vocabulary.</p>"},{"location":"tokenization/sentence_piece_unigram/#the-objective-function","title":"The Objective Function","text":"<p>Given a sentence \\(S\\) and a candidate segmentation \\(\\mathbf{x} = \\{t_1, t_2, \\dots, t_k\\}\\), the model calculates the probability of that segmentation as:</p> \\[P(\\mathbf{x}) = \\prod_{i=1}^{k} P(t_i)\\] <p>The goal of the tokenizer is to find the segmentation that maximizes this likelihood:</p> \\[\\mathbf{x}^* = \\arg\\max_{\\mathbf{x} \\in \\mathcal{S}} \\sum_{i=1}^{k} \\log P(t_i)\\] <p>Where \\(\\mathcal{S}\\) is the set of all possible segmentations. This is efficiently solved using Viterbi decoding (Dynamic Programming) in \\(O(N^2)\\) time relative to string length.</p>"},{"location":"tokenization/sentence_piece_unigram/#2-training-the-em-style-pruning-process","title":"2. Training: The EM-Style Pruning Process","text":"<p>Unigram training is a \"top-down\" approach. Instead of merging small units (like BPE), it starts with a massive vocabulary and prunes it down.</p> <ol> <li>Initialization: Start with a very large vocabulary (e.g., every character in the corpus plus the most frequent substrings).</li> <li>Expectation (E-step): Given the current vocabulary and token probabilities, find the optimal (Viterbi) segmentation for every sentence in the corpus.</li> <li>Maximization (M-step): Update the token probabilities \\(P(t_i)\\) based on their frequency in the newly computed optimal segmentations.</li> <li>Pruning:<ul> <li>For each token, calculate the loss: how much the total corpus likelihood would decrease if that token were removed from the vocabulary.</li> <li>Discard the bottom \\(X\\%\\) of tokens with the lowest loss.</li> <li>Note: To ensure every string remains segmentable, single characters are never pruned.</li> </ul> </li> <li>Iteration: Repeat until the target vocabulary size is reached.</li> </ol>"},{"location":"tokenization/sentence_piece_unigram/#3-small-intuitive-example","title":"3. Small Intuitive Example","text":"<p>Assume the vocabulary contains the following tokens with probabilities:</p> Token Probability <code>low</code> 0.30 <code>lower</code> 0.25 <code>er</code> 0.20 <code>l</code> 0.05 <code>o</code> 0.05 <code>w</code> 0.05 <p>Input word: <code>lower</code></p>"},{"location":"tokenization/sentence_piece_unigram/#possible-segmentations","title":"Possible Segmentations","text":"<ol> <li> <p><code>lower</code> \\(P = 0.25\\)</p> </li> <li> <p><code>low + er</code> \\(P = 0.30 \\times 0.20 = 0.06\\)</p> </li> <li> <p><code>l + o + w + er</code> \\(P = 0.05^3 \\times 0.20 = 0.00025\\)</p> </li> </ol>"},{"location":"tokenization/sentence_piece_unigram/#selected-tokenization","title":"Selected Tokenization","text":"<p>The tokenizer selects <code>lower</code> because it maximizes the likelihood.</p>"},{"location":"tokenization/sentence_piece_unigram/#4-the-sentencepiece-secret-sauce","title":"4. The SentencePiece \"Secret Sauce\"","text":"<p>SentencePiece is the implementation wrapper that adds critical features for modern LLMs:</p> <ul> <li>Lossless Reversibility: It replaces spaces with a meta-symbol (usually <code>_</code>). Because the space is treated as a standard character, the original string can be reconstructed perfectly (detokenized) without complex rules.</li> <li>Byte-Fallback: If the model encounters a character not in its vocabulary, it falls back to UTF-8 bytes. This effectively eliminates the \"Unknown Token\" (<code>UNK</code>) problem.</li> <li>Pre-tokenization Independence: It does not require splitting text by spaces first. This makes it natively compatible with non-segmented languages like Chinese, Japanese, or Thai.</li> </ul>"},{"location":"tokenization/sentence_piece_unigram/#5-subword-regularization-sampling","title":"5. Subword Regularization (Sampling)","text":"<p>This is a major advantage for model robustness. During training, instead of always using the \"best\" (Viterbi) segmentation, we sample different segmentations based on their probabilities:</p> \\[P(\\mathbf{x}) = \\frac{P(\\mathbf{x})^\\alpha}{\\sum_{\\mathbf{x}' \\in \\mathcal{S}} P(\\mathbf{x}')^\\alpha}\\] <p>By exposing the model to multiple ways of segmenting the same word (e.g., <code>higher</code> as <code>high + er</code> vs <code>h + igher</code>), the model becomes more robust to spelling variations and noise.</p>"},{"location":"tokenization/sentence_piece_unigram/#6-comparison-bpe-vs-unigram","title":"6. Comparison: BPE vs. Unigram","text":"Feature BPE (Byte-Pair Encoding) Unigram (SentencePiece) Logic Bottom-up (Iterative Merging) Top-down (Iterative Pruning) Philosophy Frequency-based heuristic Probabilistic Likelihood Optimality Greedy (not globally optimal) Global Optimum (via Viterbi) Regularization Difficult to implement Natively supports subword sampling Use Case GPT-family, Llama, RoBERTa T5, ALBERT, Multilingual models"},{"location":"tokenization/sentence_piece_unigram/#6-practical-limitations","title":"6. Practical Limitations","text":"<ul> <li>Complexity: Viterbi decoding is more computationally expensive than the greedy merges of BPE.</li> <li>Independence Assumption: It assumes tokens are independent, ignoring the linguistic context in which a subword appears.</li> <li>Training Time: The EM pruning process on a massive corpus is generally slower than BPE's initial merge-counting.</li> </ul>"},{"location":"tokenization/sentence_piece_unigram/#7-summary","title":"7. Summary","text":"<p>SentencePiece Unigram provides a principled, probabilistic objective for tokenization. Its ability to provide globally optimal segmentations, support subword regularization, and maintain lossless reversibility makes it a preferred choice for high-performance, multilingual Large Language Models.</p>"},{"location":"tokenization/unigram/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"tokenization/unigram/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"tokenization/unigram/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"tokenization/vocabulary_size_tradeoffs/","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"tokenization/vocabulary_size_tradeoffs/#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"tokenization/vocabulary_size_tradeoffs/#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"transformer_components/advanced_transformer_components_and_layers/","title":"Advanced transformer components and layers","text":"<p>This section covers the architectural evolutions that define the Modern Transformer, as used in Llama 3, Mistral, PaLM, GPT-4, and similar large-scale language models. Moving beyond the original 2017 Transformer is a common requirement for senior-level AI and ML engineering interviews.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#1-normalization-rmsnorm","title":"1. Normalization: RMSNorm","text":"<p>Replaces: LayerNorm (LN)</p> <p>Traditional LayerNorm centers the input by subtracting the mean and then scales it by the variance. RMSNorm (Root Mean Square Layer Normalization) simplifies this by removing the mean-centering step and only normalizing by the root mean square.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#the-math","title":"The Math","text":"\\[ \\bar{a}_i = \\frac{a_i}{\\text{RMS}(\\mathbf{a})} g_i \\quad \\text{where} \\quad \\text{RMS}(\\mathbf{a}) = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n a_i^2 + \\epsilon} \\] <p>Where:</p> <ul> <li> <p>\\(\\mathbf{a} \\in \\mathbb{R}^n\\): Input activation vector to the normalization layer, typically corresponding to a single token representation across the hidden dimension.</p> </li> <li> <p>\\(a_i\\): The \\(i\\)-th component of the input activation vector \\(\\mathbf{a}\\).</p> </li> <li> <p>\\(n\\): Dimensionality of the hidden representation, equal to the model hidden size.</p> </li> <li> <p>\\(\\text{RMS}(\\mathbf{a})\\): Root Mean Square of the activation vector, used to normalize the input magnitude without mean-centering.</p> </li> <li> <p>\\(\\epsilon\\): Small positive constant added for numerical stability to prevent division by zero, especially important for low-precision training.</p> </li> <li> <p>\\(g_i\\): Learnable scaling parameter applied element-wise after normalization. This replaces the gain parameter in LayerNorm while omitting the additive bias.</p> </li> <li> <p>\\(\\bar{a}_i\\): The normalized and rescaled output activation for the \\(i\\)-th dimension.</p> </li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways","title":"Takeaways","text":"<ul> <li>Efficiency: Eliminates mean computation and subtraction, reducing arithmetic and synchronization overhead on accelerators.</li> <li>Stability: Avoids cancellation errors from mean-centering, which is especially beneficial in low-precision FP16 and BF16 training.</li> <li>Simplicity: Removes the additive bias term since normalization already controls activation scale, leaving only a learnable gain.</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#2-structural-shift-pre-norm-architecture","title":"2. Structural Shift: Pre-Norm Architecture","text":"<p>The Change: Normalization is applied before attention or FFN blocks instead of after the residual connection.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#formulation","title":"Formulation","text":"<p>Post-Norm (Original): $$ x_{next} = \\text{Norm}(x + \\text{Sublayer}(x)) $$</p> <p>Pre-Norm (Modern): $$ x_{next} = x + \\text{Sublayer}(\\text{Norm}(x)) $$</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_1","title":"Takeaways","text":"<ul> <li>Gradient Flow: Clean residual paths enable stable training of very deep models</li> <li>Optimization Robustness: Reduces reliance on fragile learning rate warmup</li> <li>Industry Standard: Used in essentially all modern decoder-only LLMs</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#3-activation-excellence-swiglu","title":"3. Activation Excellence: SwiGLU","text":"<p>Replaces: ReLU, GELU</p> <p>SwiGLU is a gated linear unit variant that uses two linear projections, one gated by the Swish (SiLU) activation.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#the-math_1","title":"The Math","text":"\\[ \\text{SwiGLU}(x) = \\text{Swish}(xW + b) \\otimes (xV + c) \\]"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_2","title":"Takeaways","text":"<ul> <li>Higher Expressivity: Multiplicative gating enables richer feature interactions</li> <li>Better Scaling: Improves convergence and downstream quality at large scale</li> <li>Default Choice: Used in Llama, PaLM, and most modern LLM families</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#4-stability-choice-bias-free-linear-layers","title":"4. Stability Choice: Bias-Free Linear Layers","text":"<p>The Change: Remove additive bias terms from linear layers.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_3","title":"Takeaways","text":"<ul> <li>Training Stability: Reduces activation spikes in deep or wide networks</li> <li>Length Extrapolation: Improves robustness to longer sequences than seen during training</li> <li>Redundancy Removal: Bias is largely unnecessary when paired with normalization</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#5-positional-encoding-rotary-positional-embeddings-rope","title":"5. Positional Encoding: Rotary Positional Embeddings (RoPE)","text":"<p>Replaces: Absolute or learned positional embeddings</p> <p>RoPE encodes positional information by rotating query and key vectors in a complex plane.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_4","title":"Takeaways","text":"<ul> <li>Relative Positioning: Attention depends on relative token distance</li> <li>Natural Decay: Long-range attention strength decreases smoothly</li> <li>Context Extension: Enables interpolation and scaling for long context windows</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#6-efficient-attention-grouped-query-attention-gqa","title":"6. Efficient Attention: Grouped-Query Attention (GQA)","text":"<p>The Middle Ground: Between Multi-Head Attention (MHA) and Multi-Query Attention (MQA)</p> <p>In GQA, multiple query heads share a smaller set of key and value heads.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_5","title":"Takeaways","text":"<ul> <li>KV Cache Efficiency: Substantially reduces inference memory usage</li> <li>Quality Retention: Maintains accuracy close to full MHA</li> <li>Inference Speed: Critical for fast generation on modern hardware</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#7-attention-kernel-optimization-flashattention","title":"7. Attention Kernel Optimization: FlashAttention","text":"<p>What It Is: A fused attention kernel that avoids materializing the full attention matrix in memory.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_6","title":"Takeaways","text":"<ul> <li>Memory Bandwidth Optimization: Turns attention into a compute-efficient operation</li> <li>Asymptotic Improvement: Reduces memory from O(n\u00b2) to O(n)</li> <li>Production Standard: Used in nearly all state-of-the-art LLM stacks</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#8-parallel-attention-and-ffn","title":"8. Parallel Attention and FFN","text":"<p>The Change: Attention and FFN blocks are computed in parallel rather than sequentially within a transformer layer.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_7","title":"Takeaways","text":"<ul> <li>Lower Latency: Improves throughput in training and inference</li> <li>Scalability: Works well at large model scales</li> <li>Adopted By: PaLM and related architectures</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#9-context-extension-beyond-rope","title":"9. Context Extension Beyond RoPE","text":""},{"location":"transformer_components/advanced_transformer_components_and_layers/#91-rope-scaling-and-interpolation","title":"9.1 RoPE Scaling and Interpolation","text":"<ul> <li>NTK-aware scaling</li> <li>Linear or dynamic position scaling</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#92-sliding-window-attention","title":"9.2 Sliding Window Attention","text":"<ul> <li>Restricts attention to a fixed recent context</li> <li>Prevents unbounded KV cache growth</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_8","title":"Takeaways","text":"<ul> <li>These are architectural inference-time decisions</li> <li>Often combined with GQA for long-context efficiency</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#10-mixture-of-experts-moe","title":"10. Mixture of Experts (MoE)","text":"<p>What Changes: Dense FFNs are replaced with multiple expert FFNs and a learned routing mechanism.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_9","title":"Takeaways","text":"<ul> <li>Sparse Activation: Only a small subset of experts is active per token</li> <li>Efficient Scaling: Parameter count increases without proportional compute</li> <li>Systems Complexity: Routing and load balancing dominate implementation challenges</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#11-kv-cache-optimizations","title":"11. KV Cache Optimizations","text":"<p>Beyond GQA, modern systems apply several cache-level improvements:</p> <ul> <li>Reduced precision KV storage</li> <li>Prefix and prompt caching</li> <li>Cache eviction or compression strategies</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_10","title":"Takeaways","text":"<ul> <li>Inference performance is usually memory-bound</li> <li>KV cache design often dominates real-world latency</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#12-weight-tying-and-parameter-sharing","title":"12. Weight Tying and Parameter Sharing","text":"<p>What It Is: Sharing token embedding and output projection weights.</p>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#takeaways_11","title":"Takeaways","text":"<ul> <li>Improves sample efficiency</li> <li>Reduces total parameter count</li> <li>Remains standard in decoder-only LLMs</li> </ul>"},{"location":"transformer_components/advanced_transformer_components_and_layers/#summary-table","title":"\ud83d\udcca Summary Table","text":"Component Modern Standard Key Advantage Normalization RMSNorm Stable and efficient normalization Norm Placement Pre-Norm Enables deep transformer training Activation SwiGLU Improved expressivity and convergence Positional Encoding RoPE + Scaling Relative positioning and long context Attention Type GQA Efficient KV cache usage Attention Kernel FlashAttention Memory-efficient long-context attention FFN Structure Parallel FFN Reduced latency Linear Layers Bias-Free Stability at scale Context Handling Sliding Window Bounded memory growth Scaling Strategy MoE Sparse compute scaling"}]}